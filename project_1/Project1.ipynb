{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cgd2r84Ddk",
        "outputId": "215fd779-5ec0-4e61-8fa0-d6faa0cae059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -r ./requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom ChatGTT App with LangChain"
      ],
      "metadata": {
        "id": "6_XTX41o5HIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZP7tD2I4-30",
        "outputId": "b75427e7-dbc0-4210-dec8-f9c1ba8b5a66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response)\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLi2xXn_5Ftg",
        "outputId": "fd68f30f-78a0-4279-adc3-442c9d5dde9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your prompt: What do you know about Lagos, Nigeria? \n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
            "Human: What do you know about Lagos, Nigeria? \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'content': 'What do you know about Lagos, Nigeria? ', 'text': 'Lagos is the largest city in Nigeria and one of the fastest-growing cities in the world. It is known for its vibrant culture, bustling markets, and lively music scene. Lagos is also a major financial center in Africa and is home to many multinational corporations. The city has a mix of modern skyscrapers and historic buildings, as well as beautiful beaches along the Atlantic Ocean. However, like many big cities, Lagos also faces challenges such as traffic congestion and inadequate infrastructure.'}\n",
            "--------------------------------------------------\n",
            "Your prompt: quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Chat Memory Using ConversationBufferMemory"
      ],
      "metadata": {
        "id": "WtWcqL20OBdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c0a3e2d8",
        "outputId": "8e6c255e-5cfa-486c-c063-2b616ef381db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your prompt: Lagos is ....\n",
            "{'content': 'Lagos is ....', 'chat_history': [HumanMessage(content='Lagos is ....'), AIMessage(content='Lagos is a vibrant and bustling city in Nigeria known for its vibrant culture, lively nightlife, and diverse population. It is also a major economic hub in Africa.')], 'text': 'Lagos is a vibrant and bustling city in Nigeria known for its vibrant culture, lively nightlife, and diverse population. It is also a major economic hub in Africa.'}\n",
            "--------------------------------------------------\n",
            "Your prompt: and its population is ...\n",
            "{'content': 'and its population is ...', 'chat_history': [HumanMessage(content='Lagos is ....'), AIMessage(content='Lagos is a vibrant and bustling city in Nigeria known for its vibrant culture, lively nightlife, and diverse population. It is also a major economic hub in Africa.'), HumanMessage(content='and its population is ...'), AIMessage(content='Lagos has a population of over 14 million people, making it one of the largest cities in Africa.')], 'text': 'Lagos has a population of over 14 million people, making it one of the largest cities in Africa.'}\n",
            "--------------------------------------------------\n",
            "Your prompt: what is its area? \n",
            "{'content': 'what is its area? ', 'chat_history': [HumanMessage(content='Lagos is ....'), AIMessage(content='Lagos is a vibrant and bustling city in Nigeria known for its vibrant culture, lively nightlife, and diverse population. It is also a major economic hub in Africa.'), HumanMessage(content='and its population is ...'), AIMessage(content='Lagos has a population of over 14 million people, making it one of the largest cities in Africa.'), HumanMessage(content='what is its area? '), AIMessage(content='Lagos covers an area of approximately 1,171 square kilometers.')], 'text': 'Lagos covers an area of approximately 1,171 square kilometers.'}\n",
            "--------------------------------------------------\n",
            "Your prompt: What did i ask you \n",
            "{'content': 'What did i ask you ', 'chat_history': [HumanMessage(content='Lagos is ....'), AIMessage(content='Lagos is a vibrant and bustling city in Nigeria known for its vibrant culture, lively nightlife, and diverse population. It is also a major economic hub in Africa.'), HumanMessage(content='and its population is ...'), AIMessage(content='Lagos has a population of over 14 million people, making it one of the largest cities in Africa.'), HumanMessage(content='what is its area? '), AIMessage(content='Lagos covers an area of approximately 1,171 square kilometers.'), HumanMessage(content='What did i ask you '), AIMessage(content='You asked about the area of Lagos.')], 'text': 'You asked about the area of Lagos.'}\n",
            "--------------------------------------------------\n",
            "Your prompt: quit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=False)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\", \"chat_history\"],\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response)\n",
        "    print('-' * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28677aa0"
      },
      "source": [
        "## Saving Chat Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "723b59ed",
        "outputId": "a13e6467-8e51-4fa1-8d10-ba89b5841a93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your prompt: how long does it take to travel from the sun to earth \n",
            "{'content': 'how long does it take to travel from the sun to earth ', 'chat_history': [HumanMessage(content='What is lightspeed in a vacuum?'), AIMessage(content='The speed of light in a vacuum is approximately 299,792 kilometers per second or about 186,282 miles per second. This is the fastest speed at which energy or information can travel in the universe.'), HumanMessage(content='what about the light speed in water?'), AIMessage(content='The speed of light in a medium like water is slower than in a vacuum. In water, the speed of light is about 225,000 kilometers per second or roughly 140,000 miles per second. This difference in speed is due to the interactions of light with the atoms and molecules in the medium, which can slow down the propagation of light.')], 'text': 'The average distance from the Sun to Earth is about 93 million miles (150 million kilometers). Since light travels at about 186,282 miles per second in a vacuum, it would take approximately 8 minutes and 20 seconds for light to travel from the Sun to Earth. This is why we often refer to this distance as one astronomical unit (AU).'}\n",
            "--------------------------------------------------\n",
            "Your prompt: quit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=False)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "history = FileChatMessageHistory('chat_history.json')\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    chat_memory=history,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\", \"chat_history\"],\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response)\n",
        "    print('-' * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdKVCglDVioz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}